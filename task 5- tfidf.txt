--Hive--

hive> CREATE EXTERNAL TABLE spamtf(sno string,custID string,asin string,custname string,helpful string,reviewtext string,overall string,summary string,category string)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES (
"separatorChar" = ",",
"quoteChar" = "\""
)
STORED AS TEXTFILE
LOCATION '/tfidf/spam2';

hive> insert into spamtf select sno,custID,asin,custname,helpful,reviewtext,overall,summary,category from db.spam;

--Pig--

grunt> electronicsLoad = Load 'hdfs://cluster-ct-m/tfidf/spam2/000000_0' USING org.apache.pig.piggybank.storage.CSVExcelStorage(',','YES_MULTILINE') AS (sno:chararray,custID:chararray,asin:chararray,custname:chararray,helpful:chararray,reviewtext:chararray,overall:chararray,summary:chararray,category:chararray);

grunt> electdata = FOREACH electronicsLoad GENERATE sno,custID,helpful,overall,REPLACE(REPLACE(REPLACE(REPLACE(REPLACE((REPLACE(summary,'[\r\n]+','')),'<[^>]*>' , ' '),'[^a-zA-Z\\s\']+',' '),'(?=\\S*[\'])([a-zA-Z\'-]+)',''),'(?<![\\w\\-])\\w(?![\\w\\-])',''),'[ ]{2,}',' ') AS summary;

cleaned the data with below commands:
grunt> electronics_notnull = FILTER electdata BY (sno is not null);

grunt> electronics_notnull_order = ORDER electronics_notnull BY helpful DESC;

grunt> electronics_notnull_order_limit = LIMIT electronics_notnull_order 10;

grunt> electronics_top_10_users = FOREACH electronics_notnull_order_limit GENERATE sno,summary;

grunt> STORE electronics_top_10_users INTO '/pigstore2/' USING org.apache.pig.piggybank.storage.CSVExcelStorage(',','YES_MULTILINE');



hadoop jar /usr/lib/hadoop/hadoop-streaming.jar -file /home/rohit_shinde2/python2/mapper1.py /home/rohit_shinde2/python2/reducer1.py -mapper "python mapper1.py" -reducer "python reducer1.py" -input /pigstore2/part-r-00000 -output /mapred/spam/output1

hadoop jar /usr/lib/hadoop/hadoop-streaming.jar -file /home/rohit_shinde2/python2/mapper2.py /home/rohit_shinde2/python2/reducer2.py -mapper "python mapper2.py" -reducer "python reducer2.py" -input /mapred/spam/output1/part-0000* -output /mapred/spam/output2

hadoop jar /usr/lib/hadoop/hadoop-streaming.jar -file /home/rohit_shinde2/python2/mapper3.py /home/rohit_shinde2/python2/reducer3.py -mapper "python mapper3.py" -reducer "python reducer3.py" -input /mapred/spam/output2/part-0000* -output /mapred/spam/output3

hadoop jar /usr/lib/hadoop/hadoop-streaming.jar -files /home/rohit_shinde2/python2/mapper4.py -mapper "python mapper4.py" -input /mapred/spam/output3/part-0000* -output /mapred/spam/output4


hadoop fs -rm /mapred/spam/output4/_SUCCESS
hadoop fs -getmerge /mapred/spam/output4/ /home/rohit_shinde2/cleandata/spamtfidf.csv

-- Top 10 spam accounts with their tf-idf values

hive> use db;
hive> SET hive.cli.print.header =true;

hive> CREATE external TABLE IF not exists spamtfidf (word string, id string, tfidf double) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' WITH SERDEPROPERTIES ("separatorChar"="\t") LOCATION '/home/rohit_shinde2/cleandata/spamtfidf.csv' tblproperties("skip.header.line.count"="0");

hive> Load data local inpath '/home/rohit_shinde2/cleandata/spamtfidf.csv' overwrite into table spamtfidf;

hive> SELECT * FROM (SELECT id,word,tfidf, rank() over (PARTITION BY id ORDER BY tfidf DESC) as rank FROM db.spamtfidf DISTRIBUTE BY id SORT BY id desc) rk WHERE rank < 10 ORDER BY id, rank;

-- Top 10 ham accounts with their tf-idf values

hive> use db;
hive> SET hive.cli.print.header =true;

hive> CREATE external TABLE IF not exists hamtfidf (word string, id string, tfidf double) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' WITH SERDEPROPERTIES ("separatorChar"="\t") LOCATION '/home/rohit_shinde2/cleandata/hamtfidf.csv' tblproperties("skip.header.line.count"="0");

hive> Load data local inpath '/home/rohit_shinde2/cleandata/hamtfidf.csv' overwrite into table hamtfidf;

